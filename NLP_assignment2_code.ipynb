{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "      \n",
    "      \n",
    "      \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_call(data1,data2):\n",
    "  vectorizer=CountVectorizer()\n",
    "  y=vectorizer.fit_transform(data1)\n",
    "  X=vectorizer.fit_transform(data2)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=80)  #[1]\n",
    "  clfr=OneVsRestClassifier(LogisticRegression())                                              #[3] begins\n",
    "  clfr.fit(X_train, y_train)                                                                  #[3] ends                 \n",
    "  y_pred = clfr.predict(X_test)\n",
    "  acc=accuracy_score(y_test,y_pred)\n",
    "  zip1,zip2=y_test,y_pred\n",
    "  accuracies=cross_val_score(estimator=clfr,X=X_train,y=y_train,cv=10)                        #[1]\n",
    "  acc_mean=accuracies.mean()\n",
    "  return('accuracy=',acc,'accuracy_mean=',acc_mean)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('accuracy=',\n",
       " 0.5072886297376094,\n",
       " 'accuracy_mean=',\n",
       " 0.5294025157232704,\n",
       " <686x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 772 stored elements in Compressed Sparse Row format>,\n",
       " <686x12 sparse matrix of type '<class 'numpy.int32'>'\n",
       " \twith 520 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running model and experiments on \"topic.csv\"\n",
    "df=pd.read_table(r\"C:\\Users\\manpreet\\topic.csv\",sep=',')\n",
    "df1=df.loc[:,\"annotation\"]\n",
    "df2=df.loc[:,\"body\"]\n",
    "model_call(df1,df2)\n",
    "word_n_gram(df1,df2)\n",
    "remove_stop_words(df1,df2)\n",
    "stem_words(df1,df2)\n",
    "td_idf(df1,df2)\n",
    "pos_exp(df1,df2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 1 (word n-grams for n > 1) Category:N-gram specification\n",
    "\n",
    "def word_n_gram(data1,data2):\n",
    "\n",
    " bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),token_pattern=r'\\b\\w+\\b', min_df=1)       #[4]begins\n",
    " analyze = bigram_vectorizer.build_analyzer()\n",
    " analyze('Bi-grams are present!') == (['bi', 'grams', 'are', 'present', 'bi grams', 'grams are', 'are present'])\n",
    "\n",
    " vectorizer=CountVectorizer()\n",
    " y=bigram_vectorizer.fit_transform(data1)\n",
    " X=bigram_vectorizer.fit_transform(data2)                                                       #[4]ends\n",
    " X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=80) \n",
    " clfr=OneVsRestClassifier(LogisticRegression())\n",
    " clfr.fit(X_train, y_train)\n",
    "\n",
    " y_pred = clfr.predict(X_test)\n",
    " acc=accuracy_score(y_test,y_pred)\n",
    "\n",
    " accuracies=cross_val_score(estimator=clfr,X=X_train,y=y_train,cv=10)\n",
    " acc_mean=accuracies.mean()\n",
    " return('accuracy=',acc,'accuracy_mean=',acc_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 2 (Removing Stop words) category: Normalisation and weighting\n",
    "\n",
    "def remove_stop_words(data1,data2):\n",
    " import nltk\n",
    "\n",
    " from nltk.tokenize import word_tokenize,sent_tokenize\n",
    " from nltk.corpus import stopwords\n",
    " stop = stopwords.words('english')                                      #[4]begins\n",
    "\n",
    " df3 = data2.str.lower().str.split()\n",
    "\n",
    " df4=df3.apply(lambda x: [item for item in x if item not in stop])      #[4]ends\n",
    " df5=[\" \".join(title) for title in df4.values]\n",
    "\n",
    "\n",
    " return(model_call(data1,df5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 3 (Stem words) category: Normalisation and weighting\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "def stem_words(data1,data2):\n",
    " \n",
    " porter_stemmer = PorterStemmer()                                               #[5]begins\n",
    " df['split']=data2.apply(lambda x : filter(None,x.split(\" \")))\n",
    " df['stem'] = df[\"split\"].apply(lambda x: [porter_stemmer.stem(y) for y in x])   \n",
    " df['stem_join']=[\" \".join(title) for title in df['stem'].values]               #[5]ends\n",
    "\n",
    " return(model_call(data1,df['stem_join']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment4 (Td-IDF weighting) category: Normalisation and weighting\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def td_idf(data1,data2):\n",
    "\n",
    "\n",
    " vectorizer1 = TfidfVectorizer()\n",
    " X=vectorizer1.fit_transform(data2)\n",
    "\n",
    " vectorizer=CountVectorizer()\n",
    " y=vectorizer.fit_transform(data1)\n",
    "\n",
    " X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state=800)\n",
    " clfr=OneVsRestClassifier(LogisticRegression())\n",
    " clfr.fit(X_train, y_train)\n",
    " y_pred = clfr.predict(X_test)\n",
    " acc=accuracy_score(y_test,y_pred)\n",
    " accuracies=cross_val_score(estimator=clfr,X=X_train,y=y_train,cv=10)\n",
    " acc_mean=accuracies.mean()\n",
    " return('acc=',acc,'accuracy_mean=',acc_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment5 (POS tags) category:Syntax and stylometry\n",
    "def pos_exp(data1,data2)\n",
    "from nltk import pos_tag\n",
    "tagged = data2.str.split().map(pos_tag)             #[6]begins\n",
    "print(tagged.head())                                #[6]ends\n",
    " \n",
    "def count_tags(title_with_tags):\n",
    " df_tag1=[]\n",
    " for  word, tag in title_with_tags:\n",
    "  df_tag=  ' '.join(word + '_' + tag )\n",
    "  df_tag1.append(df_tag)\n",
    " return(df_tag1)\n",
    "k=tagged.map(count_tags)\n",
    " return(data1,k)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('accuracy=',\n",
       " 0.5830903790087464,\n",
       " 'accuracy_mean=',\n",
       " 0.5800353773584905,\n",
       " <686x2 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 686 stored elements in Compressed Sparse Row format>,\n",
       " <686x2 sparse matrix of type '<class 'numpy.int32'>'\n",
       " \twith 686 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Runing model and experiments over \"virality.csv\"\n",
    "\n",
    "dfv=pd.read_table(r\"C:\\Users\\manpreet\\virality.csv\",sep=',')\n",
    "dfv1=dfv['class']\n",
    "dfv2=dfv['body']\n",
    "model_call(dfv1,dfv2)\n",
    "\n",
    "td_idf(dfv1,dfv2)\n",
    "remove_stop_words(dfv1,dfv2)\n",
    "stem_words(dfv1,dfv2)\n",
    "word_n_gram(dfv1,dfv2)\n",
    "pos_exp(dfv1,dfv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#references\n",
    "#[1] 3.1. Cross-validation: evaluating estimator performance — scikit-learn 0.19.1 documentation. (2018). Retrieved from http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "#[2]  can \"OneVsRestClassifier\" be usefull to tune FMclassification into a multi-class classification case · Issue #49 · ibayer/fastFM. (2018). Retrieved from https://github.com/ibayer/fastFM/issues/49\n",
    "#[3]4.2. Feature extraction — scikit-learn 0.19.1 documentation. (2018). Retrieved from http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "#[4]Pandas, S. (2018). Stopword removal with NLTK and Pandas. Retrieved from https://stackoverflow.com/questions/33245567/stopword-removal-with-nltk-and-pandas\n",
    "#[5]word, a. (2018). apply porters stemmer to a Pandas column for each word. Retrieved from https://stackoverflow.com/questions/43795310/apply-porters-stemmer-to-a-pandas-column-for-each-word\n",
    "#[6] list?, H. (2018). How to output NLTK pos_tag in the string instead of a list?. Retrieved from https://stackoverflow.com/questions/42865623/how-to-output-nltk-pos-tag-in-the-string-instead-of-a-list\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
